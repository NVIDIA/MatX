{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Numerical Computing with MatX\n",
    "\n",
    "<img src=\"img/dli-matx-overview.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial List\n",
    "1. Introduction (this tutorial)\n",
    "2. [Operators](02_operators.ipynb)\n",
    "3. [Executors](03_executors.ipynb)\n",
    "4. [Radar Pipeline Example](04_radar_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to NVIDIA's MatX training! In this course, you will learn how to use MatX, a modern C++ header-only library designed for generic matrix and n-dimensional array (tensor) operations. By borrowing heavily from both Python and MATLAB syntax, MatX is designed to provide native CUDA performance all behind a friendly and extensible API. Whereas other GPU-accelerated numerical computing libraries like [CuPy](https://cupy.dev/) and [Numba](http://numba.pydata.org/) allow the Python developer to build GPU-centric applications, MatX extends this \"quick build, fast performance\" thesis to the C++ developer. Our ultimate goal is to encourage developer productivity and provide a quick on-ramp from prototype code to statically-typed production code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Structure\n",
    "This training contains a series of tutorials in increasing order of name that will guide you through basic and intermediate MatX features. Early tutorials are implemented using an in-line interpreter that allows you to run MatX code natively in the Jupyter cells. Later, more complex tutorials that are performance sensitive will require you to open source code in separate source files, and run it from the Jupyter notebook. As time permits, there is another notebook called `99_assignments.ipynb` that will give a series of problems to solve using MatX primitives. The assignments have verification code to make sure the answer is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Writing high-performance GPU code usually requires writing CUDA code directly and having at least a basic knowledge of the underlying hardware. Users not wanting to invest the time to learn this will typically use higher-level languages with CUDA bindings, such as MATLAB or Python. Using these high-level languages may cause interoperability with existing libraries and software more difficult. High-level languages can also cause a performance penalty for applications that require the lowest latency or highest throughput. \n",
    "\n",
    "MatX aims to provide syntax familiar to high-level languages with the performance and interoperability of C++. Most of the runtime costs are transferred into compile-time costs by using C++ templates, and kernels are auto-generated for most tensor and domain-specific functions. MatX does not require any CUDA or hardware knowledge; users can write algebraic expressions or use simple functions on tensor objects without every writing CUDA code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatX Structure\n",
    "MatX is structured into two separate, but compatible, APIs: tensor and frontend. The tensor API provides a suite of mathematical operations for operating on MatX tensor types. These operations are evaluated at compile-time and can be combined into chained expressions to improve performance. The frontend APIs are simple interfaces into numerous existing CUDA libraries. These include cuFFT, cuBLAS, cuSolver, and more. A common tensor type can be passed seamlessly between the tensor and frontend APIs, and MatX uses the information from these operations to call the appropriate kernels or libraries.\n",
    "\n",
    "One of the key concepts in MatX's design and usage is the disassociation of memory allocation from processing. We will cover this concept in depth during the training, but MatX optimizes primitive performance by discouraging operations that negatively impact performance, such as deep copies. When creating an application, the user creates views into memory regions that provide zero-cost abstractions of looking at the data differently in memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "The basic building block of all MatX operations is the tensor. Having the common tensor type for all operations provides a powerful tool for manipulating data, and more importantly, abstracts much of the complexity from the user. MatX treats tensors as multi-dimensional arrays as in both Python and MATLAB. Tensors have three main properties: rank, size, and type. The rank and type are specified at compile time, while the size can be specified at either runtime or compile-time. The rank describes how many dimensions the tensor has and can be anywhere from 0 (scalar) to N, where N is limited by the amount of memory in your system. The size of the tensor describes how many elements are in each dimension. Lastly, the type of the tensor is the type of data stored in the tensor. MatX supports all common data types supporting C++ arithmetic operators, and can be extended to user-defined types that provide the same overloads.\n",
    "\n",
    "- **Tensors**\n",
    "\n",
    "    Tensors (`tensor_t`) provide views of the underlying tensor data and optionally allocate memory. Tensors describe properties such as stride, size of each dimension, and provide basic accessor functions for retrieving the data in a view. Tensors can be created from other tensors when convenient, which allows a view into another tensor without copying any data. For example, a sliced view of a tensor (which we'll get to soon) can be used to generate two more tensors of both real and imaginary parts of a complex number. The term `view` is sometimes interchangeably used with the term `tensor` in MatX since all tensors are simply a view into underlying memory. The memory backing tensors is reference counted; the last tensor to be destroyed either explicitly or implicitly will free any non-user-managed data. Users also have the option of passing custom pointers and maintaining ownership of the pointers so that nothing is freed from MatX when the tensor goes out of scope.\n",
    "<br>\n",
    "- **Operators**\n",
    "\n",
    "    Operators are an abstract term used for types that can return a value at a given index. Currently there are three types of operators: tensors, operator expressions, and generators. Operators are covered in more detail in lesson 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Example\n",
    "In this example, we give an introduction to the MatX library by performing basic operations on tensor objects. We will show the following operations on tensors:\n",
    "\n",
    "0. Library Import\n",
    "1. Creation\n",
    "2. Initialization\n",
    "3. Permuting (Rearrange dimensions of tensor)\n",
    "4. Slicing\n",
    "5. Cloning\n",
    "\n",
    "All of these operations are on tensor object types only, and do not use any of the frontend API. If desired, open `example1.cu` in a separate tab to view the entire file. Let's walk through the example line-by-line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Library Import\n",
    "\n",
    "During this tutorial, the includes and Jupyter intepreter will enable compilation, however in deployed code, MatX is most-often compiled via the CUDA Compiler, `nvcc`. If you're curious, example command line to build and execute code can be found [here](exercises/compile_and_run.sh).\n",
    "\n",
    "During this tutorial, the includes and Jupyter interpreter will enable compilation, however in deployed code, MatX is most-often compiled via the CUDA Compiler, `nvcc`. If you're curious, example command line to build and execute code can be found [here](exercises/compile_and_run.sh).\n",
    "\n",
    "When using MatX, be sure to import the library via:\n",
    "\n",
    "```c++\n",
    "#include <matx.h>\n",
    "```\n",
    "\n",
    "If you want to leave off `matx::` in front of MatX functions, you can also use:\n",
    "\n",
    "```c++\n",
    "using namespace matx;\n",
    "```\n",
    "\n",
    "Be aware that since MatX mimics a lot of functionality from the standard library, you may see common names from MatX in your current namespace. It's usually recommended to not import the entire `matx` namespace unless absolutely necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//todo this should be moved to a hidden init block that runs automatically when the notebook starts\n",
    "#pragma cling add_library_path(\"/usr/local/cuda/lib64\")\n",
    "#pragma cling add_library_path(\"/opt/xeus/cling/lib\")\n",
    "//#pragma cling add_library_path(\"/usr/Lib/gcc/x86_64-Linux-gnu/11/\")\n",
    "#pragma cling add_library_path(\"/usr/lib/x86_64-linux-gnu/openblas64-openmp/\")\n",
    "#pragma cling add_include_path(\"/usr/local/cuda/include\")\n",
    "#pragma cling add_include_path(\"/usr/include/x86_64-linux-gnu/openblas64-openmp\")\n",
    "#pragma cling add_include_path(\"/opt/xeus/cling/tools/Jupyter/kernel/MatX/include\")\n",
    "#pragma cling add_include_path(\"/opt/xeus/cling/tools/Jupyter/kernel/MatX/build/_deps/cccl-src/libcudacxx/include\")\n",
    "//#pragma cling load(\"libgomp\")\n",
    "#pragma cling load(\"libopenblas64\")\n",
    "#pragma cling load(\"libcuda\")\n",
    "#pragma cling load(\"libcudart\")\n",
    "#pragma cling load(\"libcurand\")\n",
    "#pragma cling load(\"libcublas\")\n",
    "#pragma cling load(\"libcublasLt\")\n",
    "\n",
    "#include <cuda/std/__algorithm/max.h>\n",
    "#include <cuda/std/__algorithm/min.h>\n",
    "\n",
    "#define MATX_EN_OPENBLAS\n",
    "#define MATX_EN_OPENBLAS_LAPACK\n",
    "#define MATX_OPENBLAS_64BITINT\n",
    "\n",
    "#include \"matx.h\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creation\n",
    "\n",
    "First, we create a tensor object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto t2 = matx::make_tensor<int>({5,4});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While you can create `tensor_t` objects directly, it us advised to used `make_tensor` instead. `tensor_t` has several template parameters that may change in the future, and using `make_tensor` avoids that ambiguity and always returns the correct tensor type. One exception to this is when a user wants to use `tensor_t` as a class member variable. In that case you need to use `tensor_t` directly, and in the constructor of your class use `make_tensor` with your `tensor_t` variable as the first argument. This will be demonstrated later.\n",
    "\n",
    "`make_tensor` takes one template parameter indicating the type of the tensor, and zero or more function parameters. At a minimum, the sizes of the tensor are specified in curly braces, or in the case of a 0-D tensor, no size list is specified. For a complete guide on creating tensors in different ways, please visit: https://nvidia.github.io/MatX/creation.html.\n",
    "\n",
    "On the line above we are creating a 2D tensor (matrix) of integer values. These template parameter and size list are required for all tensor objects to allow compile-time optimizations. The rank of the tensor is deduced from the number of elements in the size list. Using normal matrix notation, this would be described as an integer matrix with 5 rows and 4 columns.\n",
    "\n",
    "**NOTE** Unlike MATLAB, MatX follows the C-style for indexing, meaning we assume row-major formats rather than column-major, and 0-based indexing rather than 1-based. \n",
    "\n",
    "By not passing extra arguments, we are asking `make_tensor` to allocate the memory using CUDA managed memory so that it is visible by both host and device code. Alternatively, users can opt to manage their own memory or have MatX allocate different types of memory using other forms of the constructor (see documentation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialization \n",
    "\n",
    "After allocating the tensor, we initialize the underlying data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[5, 4], Strides:[4,1]\n",
      "000000:  1  2  3  4 \n",
      "000001:  5  6  7  8 \n",
      "000002:  9  10  11  12 \n",
      "000003:  13  14  15  16 \n",
      "000004:  17  18  19  20 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x7f5e43dfec30\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.SetVals({  \n",
    "           {1, 2, 3, 4},\n",
    "           {5, 6, 7, 8},\n",
    "           {9, 10, 11, 12},\n",
    "           {13, 14, 15, 16},\n",
    "           {17, 18, 19, 20}\n",
    "           });\n",
    "\n",
    "t2.PrefetchDevice(0);\n",
    "\n",
    "matx::print(t2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor is initialized using a nested initializer list inside of the `SetVals` member function, specifying the values of the matrix. The initializer list is a single-nested list to match a 2D tensor shape, but this can be extended up to 4D tensors. `operator()` is also available to set and get individual values of a tensor as an alternative:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[5, 4], Strides:[4,1]\n",
      "000000:  42  2  3  4 \n",
      "000001:  5  6  7  8 \n",
      "000002:  9  10  11  12 \n",
      "000003:  13  14  117  16 \n",
      "000004:  17  18  19  20 \n",
      "My updates value for (3,2): 117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(std::basic_ostream<char, std::char_traits<char> >::__ostream_type &) @0x7f5eafc34540\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2(0,0) = 42;\n",
    "t2(3,2) = 117;\n",
    "\n",
    "matx::print(t2);\n",
    "\n",
    "std::cout << \"My updates value for (3,2): \" << t2(3,2) << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**NOTE** The syntax above is executed on the host when written. This works for any type of memory accessible by the host, but will result in a segmentation fault if the tensor is backed by device memory.\n",
    "\n",
    "The next call to `PrefetchDevice(0)` ensures that any data that may have been set on the host is now copied to the device. The `0` parameter is the CUDA stream, and for this example, we're operating only with the default stream (0). By default, prefetching will instruct the CUDA runtime to prefetch all data visible to the current view. After this step completes, the initialized data is visible on both the host and device. Note that if the user is not using managed memory, `operator()` to set values is not available on the host, and instead the values should be copied from a host-side tensor. \n",
    "\n",
    "The prefetch line can be ommited and the program will still work correctly, but the first time `t2` is accesed on the device will trigger a page fault and the data would be moved automatically.\n",
    "\n",
    "On the next line we print the size of the tensor dimensions and the current data inside the tensor:\n",
    "\n",
    "```\n",
    "print(t2);\n",
    "```  \n",
    "\n",
    "`print` is a utility function to print a tensor or operator's contents to stdout. Printing can be used with any type of operator, including ones that have no memory backing them (see upcoming generators section). If a tensor is being printed, the data backing it can reside either on the host or device, and MatX will move it before printing if needed. With no arguments `print` will print the entire contents of the tensor. However, the size of the printing can also be limited by passing a limit to each dimension. For example, `print(3,2)` would print the first 2 columns and 3 rows of the 2D tensor. The contents of the tensor printed should appear as an increasing sequence of numbers from the top to bottom rows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Permute\n",
    "The next section calls `permute` on the returned view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[4, 5], Strides:[1,4]\n",
      "000000:  42  5  9  13  17 \n",
      "000001:  2  6  10  14  18 \n",
      "000002:  3  7  11  117  19 \n",
      "000003:  4  8  12  16  20 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x7f5e43dfec30\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "auto t2p = permute(t2, {1,0});\n",
    "print(t2p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`permute` returns a view of the data with the dimensions swapped to match the order of the initializer list argument. In this case there are only two dimensions being permuted on a 2D tensor, so it's equivalent to a matrix transpose. However, `permute` can be used on higher-order tensors with the dimensions swapped in any particular order. Like printing, `permute` can work on any type of operator as input and not just tensors backed by memory. Observe the data and size of the tensor is now transposed when using this view:\n",
    "\n",
    "![Permuted/Transposed 2D Tensor](img/dli-transpose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that none of the underlying data has been modified. A permuted view simply accesses the data as if it were transposed, but the actual order of data in memory has not changed. This can be confirmed by re-printing the previous non-transposed view. By not modifying the data in memory, a permuted view *may* be slower to access than a non-permuted view with contiguous entries. In general, permuted views are useful for infrequent accesses, but if the transposed data would be accessed repeatedly it may be faster to permute the data in memory using the `transpose` operator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Slice\n",
    "The next line takes a slice of the 2D tensor by selecting a subset of data in both dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[2, 2], Strides:[4,1]\n",
      "000000:  6  7 \n",
      "000001:  10  11 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x7f5e43dfec30\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto t2s = matx::slice(t2, {1,1}, {3, 3});\n",
    "\n",
    "matx::print(t2s);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`t2s` is now a view of the same data, but starting at index 1 and ending at index 3 (exclusive) on both dimensions. This is equivalent to Python using `t2[1:3, 1:3]`. Since a new sliced view is returned, the new view will have dimensions `{2, 2}`.\n",
    "\n",
    "![2D Slice](img/dli-slice.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line shows a variant of `slice` that can reduce the dimension of an operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_1_i32: Tensor{int32_t} Rank: 1, Sizes:[5], Strides:[4]\n",
      "000000:  2 \n",
      "000001:  6 \n",
      "000002:  10 \n",
      "000003:  14 \n",
      "000004:  18 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x7f5e43dfec30\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto t1Col = matx::slice<1>(t2, {0, 1}, {matx::matxEnd, matx::matxDropDim});\n",
    "\n",
    "matx::print(t1Col);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this form of `slice` requires a template argument with the rank of the new slice. The second parameter to `slice` takes the starting index for each dimension, while the third takes the ending index. To include all values from the beginning on, a special sentinel of `matxEnd` can be used. Similarly, `matxDropDim` is used to indicate this dimension is the one being sliced (i.e. removed). In this case we are slicing the second column of the tensor and all rows, which produces a new 1D tensor containing only the second column of the original tensor. This is equivalent to `t2[:,1]` in Python. \n",
    "\n",
    "![Column Slice](img/dli-slice_col.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of slicing a single column, we can also slice a single row:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_1_i32: Tensor{int32_t} Rank: 1, Sizes:[4], Strides:[1]\n",
      "000000:  5 \n",
      "000001:  6 \n",
      "000002:  7 \n",
      "000003:  8 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x7f5e43dfec30\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto t1Row = matx::slice<1>(t2, {1, 0}, {matx::matxDropDim, matx::matxEnd});\n",
    "\n",
    "matx::print(t1Row);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Row Slice](img/dli-slice_row.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that since we reduced the dimension to a 1D tensor in both cases, printing a 1D tensor (vector) will appear the same in the direction the values are printed.\n",
    "\n",
    "### 5. Clone\n",
    "The last line shows `clone`, which replicates a operator's dimensions into a higher-rank operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[5, 4], Strides:[0,1]\n",
      "000000:  5  6  7  8 \n",
      "000001:  5  6  7  8 \n",
      "000002:  5  6  7  8 \n",
      "000003:  5  6  7  8 \n",
      "000004:  5  6  7  8 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x7f5e43dfec30\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "auto t2c_rows = matx::clone<2>(t1Row, {5, matx::matxKeepDim});\n",
    "\n",
    "matx::print(t2c_rows);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clone` is used on a 1D tensor from the output of the previous example, and replicates the data of the `t1` vector into a 2D tensor with 5 rows where all rows match the data in `t1`. Cloning does not replicate the data in memory; instead, the same elements in `t1` are accessed repeatedly when different rows are accessed. This not only saves memory, but also benefits from the caches in the GPU by not hitting different addresses in memory for the same value. \n",
    "\n",
    "In this case `clone` was being used on a 1D view from a 2D tensor data set, but similar code works on taking any dimension tensor and increasing it to a higher dimension. The increase in dimensions is not restricted to one. For example, a scalar (0D tensor) can be cloned into a 4F tensor where a single value in memory would appear as a 4D tensor.\n",
    "\n",
    "![Permuted/Transposed 2D Tensor](img/dli-clone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing which dimension is cloned, we can also take the same 1D tensor across columns.\n",
    "\n",
    "![Column Clone](img/dli-clone-col.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[4, 5], Strides:[1,0]\n",
      "000000:  5  5  5  5  5 \n",
      "000001:  6  6  6  6  6 \n",
      "000002:  7  7  7  7  7 \n",
      "000003:  8  8  8  8  8 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x7f5e43dfec30\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto t2c_columns = matx::clone<2>(t1Row, {matx::matxKeepDim, 5});\n",
    "\n",
    "matx::print(t2c_columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning, views do not modify the underlying data; they simply provide the metadata needed to access the elements. To show this, we will modify t2(1,0), which corresponds to the first value of our 1D slice, watch how multiple elements of the cloned view are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[5, 4], Strides:[4,1]\n",
      "000000:  42  2  3  4 \n",
      "000001:  10  6  7  8 \n",
      "000002:  9  10  11  12 \n",
      "000003:  13  14  117  16 \n",
      "000004:  17  18  19  20 \n",
      "tensor_1_i32: Tensor{int32_t} Rank: 1, Sizes:[4], Strides:[1]\n",
      "000000:  10 \n",
      "000001:  6 \n",
      "000002:  7 \n",
      "000003:  8 \n",
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[5, 4], Strides:[0,1]\n",
      "000000:  10  6  7  8 \n",
      "000001:  10  6  7  8 \n",
      "000002:  10  6  7  8 \n",
      "000003:  10  6  7  8 \n",
      "000004:  10  6  7  8 \n",
      "tensor_2_i32: Tensor{int32_t} Rank: 2, Sizes:[4, 5], Strides:[1,0]\n",
      "000000:  10  10  10  10  10 \n",
      "000001:  6  6  6  6  6 \n",
      "000002:  7  7  7  7  7 \n",
      "000003:  8  8  8  8  8 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x7f5e43dfec30\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2(1,0) = 10;\n",
    "matx::print(t2);\n",
    "matx::print(t1Row);\n",
    "matx::print(t2c_rows);\n",
    "matx::print(t2c_columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the first tutorial on MatX. In this tutorial, you learned the basics of accessing, slicing, permuting, and cloning a tensor. In the next example you will learn about operators and how to apply them to tensors. \n",
    "\n",
    "[Start Next Tutorial](02_operators.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++",
   "name": "cling-cpp17"
  },
  "language_info": {
   "codemirror_mode": "c++",
   "file_extension": ".c++",
   "mimetype": "text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
