{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Usage\n",
    "\n",
    "This notebook uses a custom magic command `%%run_matx` to run MatX code. This command wraps the code you write in a cell, adds the necessary includes and compiler flags, and then compiles and runs the code. All code in this notebook can be copied and pasted into your own MatX code without any additions beyond the environment setup. The magic `%%run_matx` must be at the beginning of the cell for the code to compile properly. \n",
    "\n",
    "Since MatX is a C++ template library, some of the code may take many seconds to compile. The type of CPU, CUDA version, and complexity of the example can affect compile times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatX GTC Lab Notebook\n",
    "## Tensor Creation and Memory Backing\n",
    "\n",
    "Tensors are the base class of memory backed storage in MatX. The Tensor class is highly flexible with many options for memory types, residency, and ownership. By default, tensors are allocated using CUDA managed memory so that it's available on both the host and device. This is great for quick prototyping and development, but for production code it's recommended to use device memory or pinned host memory for performance reasons. A set of utility `make_tensor` functions are provided to help streamline and simplify tensor creation rather than declaring the tensor object directly.\n",
    "\n",
    "`make_tensor` takes one template parameter indicating the type of the tensor, and zero or more function parameters. Without any parameters the tensor is considered a \"null tensor\" and has no shape or memory backing it. This is useful when declaring a tensor that will be given a shape and allocation later. The sizes of the tensor are specified in curly braces, or in the case of a 0-D tensor, an empty set of braces. For a complete guide on creating tensors in different ways, please visit: https://nvidia.github.io/MatX/basics/creation.html.\n",
    "\n",
    "MatX uses several conventions that can be different from other libraries:\n",
    "- Row-major memory layout\n",
    "- 0-based or C-style indexing\n",
    "- A rank 1 tensor is a different type entirely than a rank 2 tensor with one dimension of length 1\n",
    "\n",
    "In the following cell we demonstrate creating tensors of 0D (scalar), 1D, and 2D data. Tensors can be scaled to any arbitrary rank by adding more dimensions, and the rank is only limited by the available memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "// declare a 0D integer tensor (Scalar)\n",
    "auto t0 = matx::make_tensor<int>({});\n",
    "\n",
    "// declare a 1D integer tensor of length 4\n",
    "auto t1 = matx::make_tensor<int>({4});\n",
    "\n",
    "// declare a 2D fp32 tensor with shape 4x5 (4 rows and 5 columns)\n",
    "auto t2 = matx::make_tensor<float>({4,5});\n",
    "\n",
    "// declare tensor with user provided memory\n",
    "int *myptr = new int[4*5];\n",
    "auto t2_custom = matx::make_tensor<int>(myptr, t2.Shape());\n",
    "\n",
    "// declare tensor with shape of tensor t2\n",
    "auto t2_b = matx::make_tensor<int>(t2.Shape());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing & Assigning\n",
    "MatX provides several utilities for initializing and viewing its data inside tensors.\n",
    "\n",
    "To set a series of values explicitly the `SetVals` member function can be used by specifying values in an intializer list syntax. The initializer list uses nested braces to match the shape of the tensor and is supported up to 4D tensors. For higher ranks or very large tensors MatX provides an IO API to read in data from a file. See the [IO section](https://nvidia.github.io/MatX/api/io/index.html) for more information. `operator()` can also available to set and get individual values of a tensor as an alternative. `operator()` can both get and set individual values, but is not recommended for large tensors as setting individual values is not memory efficient. When setting values with both `SetVals` and `operator()` the memory backing the tensor must be modifiable from the host. For example, attempting to access device memory from these functions on a system without unified memory will result in undefined behavior.\n",
    "\n",
    "`print` is a utility function to print a tensor or operator's contents to stdout. Printing can be used with any type of operator, including ones that have no memory backing them. With no arguments `print` will print the entire contents of the operator. The size of the printing can also be limited by passing a limit to each dimension. For example, `print(3,2)` would print the first 2 columns and 3 rows of the 2D tensor. Unlike `SetVals` and `operator()`, `print` can be used on tensors with memory not accessible from the host. In this case a copy will be performed to the host before printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "// declare a 2D tensor of size with 4 rows and 5 columns\n",
    "auto t2 = matx::make_tensor<int>({4,5});\n",
    "\n",
    "// setVals in tensor\n",
    "t2.SetVals({\n",
    "          {1, 2, 3, 4, 5},\n",
    "          {6, 7, 8, 9, 10},\n",
    "          {11, 12, 13, 14, 15},\n",
    "          {16, 17, 18, 19, 20}\n",
    "          });\n",
    "\n",
    "// print a tensor\n",
    "matx::print(t2);\n",
    "\n",
    "// print elements of tensor. Memory MUST be host-accessible for this to work.\n",
    "std::cout << t2(0,0) << std::endl;\n",
    "\n",
    "t2(0,0) = 42;\n",
    "t2(3,2) = 117;\n",
    "\n",
    "matx::print(t2);\n",
    "\n",
    "std::cout << \"My updates value for (3,2): \" << t2(3,2) << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation Operators\n",
    "\n",
    "MatX also has a number of pre-built creation routines. For the full list, see https://nvidia.github.io/MatX/api/creation/operators/index.html.\n",
    "\n",
    "Shown below are [linspace](https://nvidia.github.io/MatX/api/creation/operators/linspace.html), [range](https://nvidia.github.io/MatX/api/creation/operators/range.html), and [ones](https://nvidia.github.io/MatX/api/creation/operators/ones.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "std::cout << \"Linspace (steps=10, start=0, stop=1)\" << std::endl;\n",
    "matx::print(matx::linspace<0>({10}, 1.f, 10.f));\n",
    "  \n",
    "std::cout << std::endl << \"Range (shape=10, first=0, step=0.1)\" << std::endl;\n",
    "matx::print(matx::range<0>({10}, 0.0f, 0.1f));\n",
    "  \n",
    "std::cout << std::endl << \"Ones (2x3)\" << std::endl;\n",
    "matx::print(matx::ones<float>({2, 3}));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01_A: Creating your first tensor\n",
    "\n",
    "Try defining a new integer tensor of size `{3, 5}` and initialize its values in increasing order from 0 to 15. Once defined, print your tensor to ensure the values are as expected. Next, updated element (1,2) to 101. Print the tensor again to ensure the update was valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "// Declare a tensor\n",
    "\n",
    "// SetVals in myTensor\n",
    "\n",
    "// Print your new tensor\n",
    "\n",
    "// Update the value at {1,1} to 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[01_A Solution](solutions/01_A.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Views\n",
    "MatX provides a powerful set of functions that enable arbitrary views into existing tensors without incuring additional memory storage or processing cost to reorganize the data. These views provide \"zero copy\" accessors to a tensor that can be used in MatX logic as if it were a real memory-backed tensor.\n",
    "\n",
    "MatX has feature parity to most operations expected in CuPy / MATLAB style environments. A full table of the translation of a given operation to its MatX equivalant can be found in our full documentation [here](https://nvidia.github.io/MatX/basics/matlabpython.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute\n",
    "`permute` returns a view of the data with the dimensions swapped to match the order of the initializer list argument. In the example below we swap our two dimensions, equivalent to a matrix transpose. However, `permute` can be used on higher-order tensors with the dimensions swapped in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "// declare a 2D tensor of size with 4 rows and 5 columns\n",
    "auto t2 = matx::make_tensor<int>({4,5});\n",
    "\n",
    "// setVals in tensor\n",
    "t2.SetVals({\n",
    "            {1, 2, 3, 4, 5},\n",
    "            {6, 7, 8, 9, 10},\n",
    "            {11, 12, 13, 14, 15},\n",
    "            {16, 17, 18, 19, 20}\n",
    "          });  \n",
    "\n",
    "// base tensor\n",
    "matx::print(t2);\n",
    "\n",
    "// Permute axes 0 and 1 of the tensor\n",
    "auto t2p = matx::permute(t2, {1,0});\n",
    "\n",
    "// print the permuted tensor to show the transposed data\n",
    "matx::print(t2p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice\n",
    "`slice` provides a view of a subset of data in a tensor, allowing that subset to be used and manipulated as a new operator. The `slice` utility function takes the input operator and two to three initilization lists to define the range of the provided input operator the slice will container. The ranges are defined with the start index and end (exclusive) index, and optional strides. The sentinel value `matxEnd` can be used to indicate the end of the tensor rather than specifying its length.\n",
    "\n",
    "in the example below, `t2s` will correspond to the elements [`1:2,1:5`] of the larger t2 tensor\n",
    "\n",
    "![2D Slice](img/dli-slice.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "// declare a 2D tensor of size with 4 rows and 5 columns\n",
    "auto t2 = matx::make_tensor<int>({4,5});\n",
    "\n",
    "// setVals in tensor\n",
    "t2.SetVals({\n",
    "            {1, 2, 3, 4, 5},\n",
    "            {6, 7, 8, 9, 10},\n",
    "            {11, 12, 13, 14, 15},\n",
    "            {16, 17, 18, 19, 20}\n",
    "          });  \n",
    "\n",
    "// slice example 1: same Rank\n",
    "auto t2s = matx::slice(t2, {1,1}, {3, matx::matxEnd});\n",
    "\n",
    "// print the sliced tensor to show the subset of data\n",
    "matx::print(t2s);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To reduce the rank when slicing, `slice` can be used with a template parameter to define an operator of a lower rank (dimensionality) than the input tensor. This is useful for situations like selecting a row of a matrix, for example. In the second example, we demonstrate slicing the 0th column from the t2 tensor.\n",
    "\n",
    "![Column Slice](img/dli-slice_col.png)\n",
    "\n",
    "MatX also includes several helper defines to make tensor bound definitions easier. The sentinel value `matxDropDim` is used to indicate this dimension is the one being sliced (i.e. removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "// declare a 2D tensor of size with 4 rows and 5 columns\n",
    "auto t2 = matx::make_tensor<int>({4,5});\n",
    "\n",
    "// setVals in tensor  \n",
    "t2.SetVals({\n",
    "            {1, 2, 3, 4, 5},\n",
    "            {6, 7, 8, 9, 10},\n",
    "            {11, 12, 13, 14, 15},\n",
    "            {16, 17, 18, 19, 20}\n",
    "          });  \n",
    "\n",
    "// slice example 2: Select all values of column 1\n",
    "auto t1Col = matx::slice<1>(t2, {0, 1}, {matx::matxEnd, matx::matxDropDim});\n",
    "\n",
    "// print the sliced tensor to show the subset of data\n",
    "matx::print(t1Col);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone\n",
    "`clone` provides a utlity function to expand a smaller rank operator to a larger rank by replicating the original data. For example, a 1D Tensor can be cloned to create a 2D or higher rank tensor. Cloning does not copy or replicate the original data, but rather creates a new operator that references the same memory.\n",
    "\n",
    "In the clone example below, we will take the t1Col from our previous operation, and clone it to build a 2D [5,4] tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "// declare a 2D tensor of size with 4 rows and 5 columns\n",
    "auto t2 = matx::make_tensor<int>({4,5});\n",
    "\n",
    "// setVals in tensor  \n",
    "t2.SetVals({\n",
    "            {1, 2, 3, 4, 5},\n",
    "            {6, 7, 8, 9, 10},\n",
    "            {11, 12, 13, 14, 15},\n",
    "            {16, 17, 18, 19, 20}\n",
    "          });  \n",
    "\n",
    "// slice example 2: reduce rank requires template parameter\n",
    "auto t1Col = matx::slice<1>(t2, {0, 1}, {matx::matxEnd, matx::matxDropDim});\n",
    "\n",
    "// clone the sliced 1D tensor to create a new 2D tensor\n",
    "auto t2c_cols = matx::clone<2>(t1Col, {5, matx::matxKeepDim});\n",
    "\n",
    "// print the cloned tensor to show the expanded data\n",
    "matx::print(t2c_cols);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data Backing\n",
    "We established earlier that views are not new data, but accessors into the original operator. This is a powerful tool when operating on the core data, but it's also different from some other languages where the programmer has no control over whether the data is copied. This also means that any changes to the original tensor will be reflected in all views of that tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx \n",
    "\n",
    "// declare a 2D tensor of size with 4 rows and 5 columns\n",
    "auto t2 = matx::make_tensor<int>({4,5});\n",
    "\n",
    "// setVals in tensor  \n",
    "t2.SetVals({\n",
    "            {1, 2, 3, 4, 5},\n",
    "            {6, 7, 8, 9, 10},\n",
    "            {11, 12, 13, 14, 15},\n",
    "            {16, 17, 18, 19, 20}\n",
    "          });  \n",
    "\n",
    "// slice example 2: reduce rank requires template parameter\n",
    "auto t1Col = matx::slice<1>(t2, {0, 1}, {matx::matxEnd, matx::matxDropDim});\n",
    "// clone the sliced 1D tensor to create a new 2D tensor\n",
    "auto t2c_cols = matx::clone<2>(t1Col, {5, matx::matxKeepDim});\n",
    "\n",
    "\n",
    "// modify the original tensor\n",
    "t2(0,1) = 10;\n",
    "// print our views to show the updated values\n",
    "matx::print(t2);\n",
    "matx::print(t1Col);\n",
    "matx::print(t2c_cols);\n",
    "\n",
    "// modify the tensor through a view\n",
    "t1Col(1) = 203;\n",
    "std::cout << \"------------------- After 203 -------------------\" << std::endl;\n",
    "\n",
    "// print our views to show the updated values\n",
    "matx::print(t2);\n",
    "matx::print(t1Col);\n",
    "matx::print(t2c_cols);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01_B: Operator Views\n",
    "Let's demonstrate your new skills in creating views of a tensor. Using the pre-defined `baseTensor2D`, please create the following views:\n",
    "\n",
    "- The complete first row of the `baseTensor2D`\n",
    "- A 2D square of 4 elements, composed of the first 2 rows and 2 columns of data\n",
    "- Modify the (1,1) element of baseTensor2D through the view corresponding to assign it the value of 87.\n",
    "\n",
    "Print the output at each stage to ensure your views are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "// Make tensor\n",
    "auto baseTensor2D = matx::make_tensor<int>({3,5});\n",
    "baseTensor2D.SetVals({\n",
    "  {1, 2, 3, 4, 5},\n",
    "  {6, 7, 8, 9, 10},\n",
    "  {11, 12, 13, 14, 15}\n",
    "});\n",
    "\n",
    "\n",
    "// Slice the first row of baseTensor\n",
    "\n",
    "// Create a 2D square of 4 elements, composed of the first 2 rows and 2 columns of data\n",
    "\n",
    "// Assign the value 87 to the (0,1) element of baseTensor2D and observe the change in each view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[01_B Solution](solutions/01_B.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## MatX Operators\n",
    "All of the examples above show how to view the operator's data differently, but no manipulation was done to the data. To manipulate the data, we use operators to define what work is to be done, and then call `run` to execute the operation (more on this later).\n",
    "\n",
    "Operators in MatX are an abstract type that follow the [operator interface](https://nvidia.github.io/MatX/basics/concepts.html#operator). The operator interface dictates a small number of methods that must be implemented to be used in MatX expressions. Everything from tensors to `operator+` are considered operators in MatX. Every operator in MatX except for tensors are lazy evaluated, meaning that the operation is not performed until the statement is executed. For example, the expression `C = A + B` will not perform the addition or assignment until the `run` function is called.\n",
    "\n",
    "Most operators come in unary types for operating on a single input or a binary type for operating on two inputs, but operators can be defined for an arbitrary number of inputs. Advanced users can also define their own operators. MatX supports most of the standard unary operators a user would expect from a library like NumPy. Broadcasting of operators is also supported, which allows for operations between tensors of different ranks.\n",
    "\n",
    "Operator expressions follow the normal type promotion rules of C++ and any type errors or warnings will be reported at compile time.\n",
    "\n",
    "Below we'll demonstrate both scalar and matrix support for the basic unary operators (`+`, `-`, `x`, `/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "auto A = matx::make_tensor<float>({2, 3});\n",
    "auto B = matx::make_tensor<float>({2, 3});\n",
    "auto C = matx::make_tensor<float>({2, 3});\n",
    "auto D = matx::make_tensor<float>({2, 2});\n",
    "\n",
    "A.SetVals({ {1.f, 2.f, 3.f},\n",
    "            {4.f, 5.f, 6.f}\n",
    "          });\n",
    "\n",
    "(B = A).run(); // `run` will be discussed in more detail later\n",
    "\n",
    "matx::print(A);\n",
    "matx::print(B);\n",
    "std::cout << \" val: \" << A(0,0) << std::endl;\n",
    "\n",
    "// Addition\n",
    "matx::print(A + 5.0f); // Broadcasting a scalar to a matrix\n",
    "matx::print(A + B);    // Element-wise addition of two matrices\n",
    "\n",
    "// Subtraction\n",
    "matx::print(A - 5.0f);\n",
    "matx::print(A - B);\n",
    "\n",
    "// Multiplication\n",
    "matx::print(A * 5.0f);\n",
    "matx::print(A * B);\n",
    "\n",
    "// Division\n",
    "matx::print(A / 5.0f);\n",
    "matx::print(A / B);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01_C: Operators\n",
    "Please use the provided A and B tensors to complete the following set of operations:\n",
    "\n",
    "- Multiply `A` by its scalar weight factor `aScale` to populate tensor `C`\n",
    "- In-place subtract `bOffset` from the matrix `B`\n",
    "- Add the `A` and `B` Tensors to populate tensor `D`\n",
    "\n",
    "Keep in mind that rather than storing the result of an expression in a tensor, you may pass it to the `print` function directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "auto A = matx::make_tensor<int>({2, 3});\n",
    "auto B = matx::make_tensor<int>({2, 3});\n",
    "auto C = matx::make_tensor<int>({2, 3});\n",
    "auto D = matx::make_tensor<int>({2, 2});\n",
    "\n",
    "A.SetVals({ {1, 2, 3},\n",
    "            {4, 5, 6}\n",
    "          });\n",
    "\n",
    "(B = A).run();\n",
    "\n",
    "int aScale = 5;\n",
    "int bOffset = 2;\n",
    "\n",
    "// Scale A by aScale\n",
    "\n",
    "// Subtract B by bOffset\n",
    "\n",
    "// Add A and B Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[01_C Solution](solutions/01_C.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Operators\n",
    "Generators are a type of operator that can generate values without another tensor or operator as input. For example, an identity function can generate a list of ones on the diagonal and zeros elsewhere. Generators are efficient since they require no memory and typically reduce to either a constant or an equation in the emitted code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "auto A = matx::make_tensor<float>({2, 3});\n",
    "auto H = matx::make_tensor<float>({10});\n",
    "\n",
    "// random\n",
    "(A = 0).run();\n",
    "(A = matx::random<float>(A.Shape(), matx::NORMAL)).run();\n",
    "matx::print(A);\n",
    "\n",
    "// eye\n",
    "(A = matx::eye(A.Shape())).run();\n",
    "matx::print(A);\n",
    "\n",
    "// hamming\n",
    "(H = matx::hamming<0>(H.Shape())).run();\n",
    "matx::print(H);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatX Transform Operators\n",
    "Transform operators take one or more inputs and call a backend library or kernel to manipulate the data. FFTs, GEMMs, and linear solvers are all types of transform operators. Compared to non-transform operators, transforms typically use some temporary memory and require synchronization that an element-wise operator does not. Transforms are allowed to be used in all of the same contexts that other operators are used in. For example, `C = A * matmul(A, B)` mixes both a transform operator (`matmul`) and an element-wise operator (`*`). If necessary, MatX will asynchronously allocate temporary memory for the output of the transform and free it after the operation is complete.\n",
    "\n",
    "Transform operators operate over a fixed number of dimensions, and anything higher dimensions will be batched. For example, if a 4D tensor is passed into a GEMM the left-most two dimensions are batched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication (GEMM)\n",
    "The `matmul` operator performs the matrix-matrix multiply of $$C = {\\alpha}A * B + {\\beta}C$$ where `A` is of dimensions `MxK`, `B` is `KxN`, and `C` is `MxN`. We first populate the `A` and `B` matrices with random values before the multiply, then the matrix multiply is performed. The `random` operator is used to populate the tensor with random values from a chosen distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "auto A = matx::make_tensor<float>({4, 8});\n",
    "auto B = matx::make_tensor<float>({8, 16});\n",
    "auto C = matx::make_tensor<float>({4, 16});\n",
    "\n",
    "(A = matx::random<float>(A.Shape(), matx::NORMAL)).run();\n",
    "(B = matx::random<float>(B.Shape(), matx::NORMAL)).run();\n",
    "matx::print(A);\n",
    "matx::print(B);\n",
    "\n",
    "(C = matx::matmul(A, B)).run();\n",
    "matx::print(C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Fourier Transform (FFT)\n",
    "The `fft` operator performs a fast Fourier transform on the input operator. MatX supports both 1D and 2D FFTs via the `fft` and `fft2` functions, and their inverses `ifft` and `ifft2`. For a 1D FFT anything above the first dimension is batched, while for a 2D FFT anything above the second dimension is batched. B complex and real inputs are supported, and the user is required to size the outputs appropriately for each case. For documentaiton on the FFT please see the [FFT documentation](https://nvidia.github.io/MatX/api/dft/fft/fft.html). \n",
    "\n",
    "When using complex types in MatX the CCCL library is used for both CPU and GPU compatibility with `cuda::std::complex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "using complex = cuda::std::complex<float>;\n",
    "\n",
    "auto A = matx::make_tensor<complex>({8});\n",
    "auto B = matx::make_tensor<complex>({8});\n",
    "\n",
    "(A = matx::random<complex>(A.Shape(), matx::NORMAL)).run();\n",
    "matx::print(A);\n",
    "\n",
    "(B = fft(A)).run(); // Take forward FFT of A\n",
    "matx::print(B);\n",
    "\n",
    "(A = ifft(B)).run(); // Inverse FFT of B. Result should closely match the original A input\n",
    "matx::print(A);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions\n",
    "Reductions are a class of algorithms that reduce a number of inputs into one or more outputs. Examples of reductions include summing all elements of a tensor, finding the maximum or minimum value, or counting the number of non-zero elements. Reduction functions in MatX use similar names to their counterparts in NumPy and MATLAB, such as `sum`, `min`, `max`, `mean`, `any`, and `all`. By default a reduction operator will reduce over all elements of the tensor, but a list of axes can be specified to reduce over different dimensions.\n",
    "\n",
    "Below is a simple example for calcluate a full reduction of the max and sum of our A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "auto A = matx::make_tensor<float>({2, 3});\n",
    "auto max_all = matx::make_tensor<float>({});\n",
    "auto sum_all = matx::make_tensor<float>({});\n",
    "\n",
    "// Since we are taking the max of each column, our output tensor is 1D with the same number of elements as the number of columns in A\n",
    "auto max_col = matx::make_tensor<float>({3});  \n",
    "\n",
    "(A = matx::random<float>(A.Shape(), matx::NORMAL)).run();\n",
    "\n",
    "// Max of data\n",
    "(max_all = matx::max(A)).run();\n",
    "// Min of data\n",
    "(sum_all = matx::sum(A)).run();\n",
    "// Max of each column\n",
    "(max_col = matx::max(A, {0})).run();  \n",
    "\n",
    "printf(\"A:\\n\");\n",
    "matx::print(A);\n",
    "\n",
    "printf(\"Max: %f\\n\", max_all());\n",
    "printf(\"Sum: %f\\n\", sum_all());\n",
    "printf(\"Max Col: \\n\");\n",
    "matx::print(max_col);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Additional Transforms\n",
    "MatX Supports a wide range of transforms, including both sparse and dense solvers, tensor constractions, and more. Please review the [MatX documentation](https://nvidia.github.io/MatX/api/index.html) for an exhaustive list of supported operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Transforms and generators:\n",
    "\n",
    "For this example we will generate random data to verify the distribution of our generator functions. Please implement the following:\n",
    "\n",
    "- Generate three floating point 3D tensors with sizes 2x4x8, 2x8x8, and 2x4x8\n",
    "- Populate the first two tensors with random values from a uniform distribution\n",
    "- Perform a batched matrix multiply of the first two tensors and store the output in the third tensor\n",
    "- Find the minimum values of each inner matrix of C (there should be 2 of them) and print the results\n",
    "\n",
    "Ensure that the minimum values printed match what you would expect in the third tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "// Create floating point tensors with specified sizes\n",
    "\n",
    "// Generate random data\n",
    "\n",
    "// Perform matmul and print\n",
    "\n",
    "// Find min values and print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[01_D Solution](solutions/01_D.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executors\n",
    "\n",
    "In the examples above the `run()` statement executed the operator, but it was not specified *where* the operator was executed. The `run()` statement takes an optional executor argument that allows the user to specify where the operator should be executed. Without any parameters `run()` will executor using the CUDA executor on the default stream, but in practice specifying the executor explicitly is recommended.\n",
    "\n",
    "Executors in MatX are a powerful tool for executing the same operator in many contexts. A context can currently be a single GPU, CPU, but may be extended in the future for more powerful types. MatX attempts to achieve feature parity between the executors, regardless of the performance. For example, running an FFT on the CUDA executor will use the cuFFT library on the backend, while launching the same operator on the CPU will use the optional FFTW library. Like all operators, the user must be aware of memory locality as it relates to the executor. For example, passing a tensor to a CUDA executor with malloc'd host memory on platforms without HMM will result in undefined behavior. Using CUDA managed memory works on all current executors without worrying about locality, but may come at a small performance cost.\n",
    "\n",
    "The example below the same operator is executed separately on a CUDA and Host executor. The results should match very closely, but may not match exactly due to how floating point is treated on different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "\n",
    "auto cuda_exec = matx::CUDAExecutor();\n",
    "auto host_exec = matx::HostExecutor();\n",
    "\n",
    "auto A = matx::make_tensor<float>({2, 3}); // Allocated using CUDA managed memory\n",
    "auto B = matx::make_tensor<float>({2, 3}); // Allocated using CUDA managed memory\n",
    "auto C = matx::make_tensor<float>({2, 3}); // Allocated using CUDA managed memory\n",
    "\n",
    "(A = matx::random<float>(A.Shape(), matx::NORMAL)).run(cuda_exec); // Run on GPU\n",
    "(B = matx::random<float>(B.Shape(), matx::NORMAL)).run(host_exec); // Run on CPU\n",
    "\n",
    "matx::print(A);\n",
    "matx::print(B);\n",
    "\n",
    "auto assign_op = (C = A + B); // Operator representing the computation C = A + B\n",
    "\n",
    "assign_op.run(cuda_exec); // Run on GPU\n",
    "matx::print(C);\n",
    "\n",
    "assign_op.run(host_exec); // Run on CPU\n",
    "matx::print(C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've completed the introduction to MatX tutorial and are ready to move on to the [fusion lab](02_lab_fusion.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
