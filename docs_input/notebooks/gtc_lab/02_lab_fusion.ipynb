{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators and Lazy Evaluation\n",
    "When writing a simple arithmetic expression like the following:\n",
    "\n",
    "```A = B * (cos(C) / D```\n",
    "\n",
    "Using the typical order of operations rules, we evaluate the expression in parentheses first `(cos(C) / D)`, followed by the multiply `*B1`, then the assignment `A=`. Written using standard C++ operator overloading, we would have a cosine, division, multiplication, and assignment overload. Each operator performs their respective task, then returns the value computed. That returned value is stored somewhere (either out to memory or possible in a register), then the next operator uses that output as input into its own computation. Finally, the assignment writes the value, usually out to memory.\n",
    "\n",
    "To avoid overhead of repeated accesses to global memory and multiple discrete operation calls, MatX uses a technique called **lazy evaluation** to reduce the total number of loads and stores. It does this by overloading each operator so that **instead of performing the operation, such as multiplication, instead it returns an object that represents multiplication when it’s needed.** The entire expression then generates a single type in C++ representing the full equation above, and when we ask for element (0,0) of A above, the value is computed on-the-fly without storing any values. This also implies that you can store an entire expression into a variable and nothing will be exectuted:\n",
    "\n",
    "`auto op = (B * (cos(C) / D));`\n",
    "\n",
    "In the example above op is not evaluated at creation, but is instead a handle ot the operator that can calculate the result of the equation onthe right hand side.\n",
    "\n",
    "This operator can then be further combined with other expressions, which can increase code readability without loss of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executors\n",
    "Operators are used in conjunction with executors and the ``run()`` syntax to dictate when a given operator is executed and on what acceleration hardware. Notebook 1 exploited automation in the `matx::print` function to ensure operators are called and copied to the host to facilitate printing. This is convenient, but not realistic or performant to use in a real application. \n",
    "\n",
    "Executors are types that describe how to execute an operator expression or transform. They are similar to C++’s execution policy, and may even use C++ execution policies behind the scenes. Executors are designed so that the code can remain unchanged while executing on a variety of different targets. For these notebooks, a single executor `exec` is created at the top of each notebook, and then used throughout. \n",
    "\n",
    "to exectuor work on a give executor, you can simply call the `run()` function on the operator you would like to execute, with the executor that you would like to do the work.\n",
    "\n",
    "```\n",
    "\n",
    "(A = B * (cos(C) / D)).run(exec); // immediate evaluation of fused operators into memory-backed tensor\n",
    "\n",
    "auto  myOp = B * (cos(C) / D);    // define lay operator with fused operator\n",
    "(A = myOp).run(exec);             // evaluate operaeter to a memory-backed tensor\n",
    "\n",
    "(A2 = myOp * C).run(exec);        // combine op with other tensors \n",
    "(A3 = myOp * myOp).run(exec)      // combine op with other ops\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Example\n",
    "\n",
    "Below we will take the example operation from above `(A = B * (cos(C) / D)).run();` and express it in MatX as individual operations, and as a single fused operation to demonstrate the value of the speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "auto exec = matx::CUDAExecutor();\n",
    "\n",
    "matx::index_t size_x = 128;\n",
    "matx::index_t size_y = 256;\n",
    "\n",
    "auto A      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto B      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto C      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto D      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto result = matx::make_tensor<float>({size_x, size_y});\n",
    "\n",
    "// ---- populate the data ---- //\n",
    "(A = matx::random<float>(A.Shape(), matx::NORMAL)).run();\n",
    "(B = matx::random<float>(B.Shape(), matx::NORMAL)).run();\n",
    "(C = matx::random<float>(C.Shape(), matx::NORMAL)).run();\n",
    "(D = matx::random<float>(D.Shape(), matx::NORMAL)).run();\n",
    "(result = matx::zeros({size_x, size_y})).run(exec);\n",
    "exec.sync();\n",
    "\n",
    "\n",
    "// ---- first individual, independent kernels ---- //\n",
    "exec.start_timer();\n",
    "(result = cos(C)).run(exec);     \n",
    "(result = result / D).run(exec); \n",
    "(result = result * B).run(exec);   \n",
    "exec.stop_timer();\n",
    "\n",
    "std::cout <<\"Unfused time: \" << exec.get_time_ms() << \" ms\" << std::endl;\n",
    "\n",
    "// ---- fused operation ---- //\n",
    "exec.start_timer();\n",
    "(A = B * cos(C)/D).run(exec);\n",
    "exec.stop_timer();\n",
    "std::cout <<\"fused time: \" << exec.get_time_ms() << \" ms\" << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Improvements\n",
    "The fused results in all of the performance benefits we described above:\n",
    "- a single kernel is submitted to the GPU to complete all operations\n",
    "- memory is only read from global once and written to global once\n",
    "\n",
    "This results in significant performance improvements, both for launch latency, and GPU kernel exection\n",
    "# <img src=\"img/dli-fusion.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "source": [
    "## Fusion with Operators\n",
    "\n",
    "Fusion is intuitive when all operands can be combined into a single statement (like above), and follows the natural pattern most programs would follow. The reality is often different for more complex algorithms, and this is where fusion can also provide significant benefit for readability and reuse in implementations where very complex terms are defined, in addition to the performance benefits we just showed. \n",
    "\n",
    "Combining the lazy evaluation of operators with the ability to combine operators, terms can be defined to clearly construct the specfic math for each term, and then combined later to create the complete final expression for execution.\n",
    "\n",
    "Below we show a more complex operation comprised of both unary operators and transforms, and how we can break down a very complex expression into simple terms that can be reused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise: Fusion Basics\n",
    "\n",
    "Use the following equations to create an implemention that utilizes fusion and reuse optimize underlying code. \n",
    "\n",
    "`result = A*C + B/D + ((D-C)/B)/(A*C) `\n",
    "\n",
    "An exmaple implementation is given with all operations done individually; how much faster can you make it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "auto exec = matx::CUDAExecutor();\n",
    "\n",
    "matx::index_t size_x = 128;\n",
    "matx::index_t size_y = 256;\n",
    "\n",
    "auto A      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto B      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto C      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto D      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto result = matx::make_tensor<float>({size_x, size_y});\n",
    "\n",
    "// ---- populate the data ---- //\n",
    "(A = matx::random<float>(A.Shape(), matx::NORMAL)).run();\n",
    "(B = matx::random<float>(B.Shape(), matx::NORMAL)).run();\n",
    "(C = matx::random<float>(C.Shape(), matx::NORMAL)).run();\n",
    "(D = matx::random<float>(D.Shape(), matx::NORMAL)).run();\n",
    "(result = matx::zeros({size_x, size_y})).run(exec);\n",
    "exec.sync();\n",
    "\n",
    "// ---- Reference Implementation ---- //\n",
    "exec.start_timer();\n",
    "(result = A*C).run(exec);\n",
    "(result += B/D).run(exec);\n",
    "(result += ((D-C)/B)/(A*C)).run(exec);\n",
    "exec.stop_timer();\n",
    "std::cout <<\"Separate Operators Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;\n",
    "\n",
    "// ---- Exercise: Implementation ---- //\n",
    "exec.start_timer();\n",
    "//\n",
    "// Your implementation here:\n",
    "//\n",
    "exec.stop_timer();\n",
    "std::cout <<\"Exercise Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "auto exec = matx::CUDAExecutor();\n",
    "\n",
    "matx::index_t size_x = 128;\n",
    "matx::index_t size_y = 256;\n",
    "\n",
    "auto A      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto B      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto C      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto D      = matx::make_tensor<float>({size_x, size_y});\n",
    "auto result = matx::make_tensor<float>({size_x, size_y});\n",
    "\n",
    "// ---- populate the data ---- //\n",
    "(A = matx::random<float>(A.Shape(), matx::NORMAL)).run();\n",
    "(B = matx::random<float>(B.Shape(), matx::NORMAL)).run();\n",
    "(C = matx::random<float>(C.Shape(), matx::NORMAL)).run();\n",
    "(D = matx::random<float>(D.Shape(), matx::NORMAL)).run();\n",
    "(result = matx::zeros({size_x, size_y})).run(exec);\n",
    "exec.sync();\n",
    "\n",
    "// ---- all crammed together ---- //\n",
    "exec.start_timer();\n",
    "(result = A * C  + B / D + ((D - C) / B) / A * C).run(exec); \n",
    "exec.stop_timer();\n",
    "std::cout <<\"One Equation Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;\n",
    "\n",
    "// ---- ideal implementation with reuse of operators ---- //\n",
    "exec.start_timer();\n",
    "auto term1 = A * C; \n",
    "auto term2 = B / D;\n",
    "auto term3 = (D - C) / B;\n",
    "auto term4 = term3 / term1;\n",
    "(result = term1 + term2 + term4).run(exec);\n",
    "exec.stop_timer();\n",
    "std::cout <<\"Fused Operation Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Black Scholes Fusion\n",
    "\n",
    "The Black Scholes model provides a fantastic example of a real-world set of equations that greatly benefits from operator fusion. Black Scholes provides both a complex set of expressions that provide significant readability improvements if expressed as individual expressions, but also benefits from fusion of its separate operational parts. Below is a brief description of the Black Scholes models and its composite terms:\n",
    "\n",
    "\n",
    "$$\n",
    "C(S_0, K, T) = S_0 \\,\\Phi\\bigl(d_1\\bigr) \\;-\\; K \\, e^{-rT} \\,\\Phi\\bigl(d_2\\bigr),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "d_1 = \\frac{\\ln\\!\\bigl(\\tfrac{S_0}{K}\\bigr) + \\bigl(r + \\tfrac{\\sigma^2}{2}\\bigr)T}{\\sigma \\sqrt{T}},\n",
    "\\quad\n",
    "d_2 = d_1 - \\sigma \\sqrt{T}.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- \\( S_0 \\) is the current stock price\n",
    "- \\( K \\) is the strike price\n",
    "- \\( T \\) is the time to maturity (in years)\n",
    "- \\( r \\) is the risk-free interest rate (annualized)\n",
    "- \\( \\sigma \\) is the volatility of the underlying stock (annualized)\n",
    "- \\( \\Phi(\\cdot) \\) is the cumulative distribution function (CDF) of the standard normal distribution\n",
    "\n",
    "\n",
    "\n",
    "We can easily translate this by expressing each of the terms defined above as separate MatX operators, then fusing the execution of those operators in the final run command.\n",
    "\n",
    "Try breaking the equation below into the following operators:\n",
    "\n",
    "```\n",
    "VsqrtT  = V * sqrt(T);\n",
    "d1      = (log(S / K) + (r + 0.5 * V * V) * T) / VsqrtT ;\n",
    "d2      = d1 - VsqrtT;\n",
    "cdf_d1  = normcdf(d1);\n",
    "cdf_d2  = normcdf(d2);\n",
    "expRT   = exp(-1 * r * T); \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "auto exec = matx::CUDAExecutor();\n",
    "\n",
    "using dtype = double;\n",
    "matx::index_t input_size = 100;\n",
    "\n",
    "// ---- declare input data ---- //\n",
    "auto K = matx::make_tensor<dtype>({input_size});\n",
    "auto S = matx::make_tensor<dtype>({input_size});\n",
    "auto V = matx::make_tensor<dtype>({input_size});\n",
    "auto r = matx::make_tensor<dtype>({input_size});\n",
    "auto T = matx::make_tensor<dtype>({input_size});\n",
    "auto output = matx::make_tensor<dtype>({input_size});  \n",
    "\n",
    "// ---- populate the data ---- //\n",
    "(K = matx::random<float>(K.Shape(), matx::NORMAL)).run();\n",
    "(S = matx::random<float>(S.Shape(), matx::NORMAL)).run();\n",
    "(V = matx::random<float>(V.Shape(), matx::NORMAL)).run();\n",
    "(r = matx::random<float>(r.Shape(), matx::NORMAL)).run();\n",
    "(T = matx::random<float>(T.Shape(), matx::NORMAL)).run();\n",
    "(output = matx::zeros({input_size})).run(exec);\n",
    "exec.sync();\n",
    "\n",
    "// ---- Exercise: Implementation ---- //\n",
    "exec.start_timer();\n",
    "//\n",
    "// Your implementation here:\n",
    "//\n",
    "exec.stop_timer();\n",
    "std::cout <<\"Exercise Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_matx\n",
    "auto exec = matx::CUDAExecutor();\n",
    "\n",
    "using dtype = double;\n",
    "matx::index_t input_size = 100;\n",
    "\n",
    "\n",
    "// ---- declare input data ---- //\n",
    "auto K = matx::make_tensor<dtype>({input_size});\n",
    "auto S = matx::make_tensor<dtype>({input_size});\n",
    "auto V = matx::make_tensor<dtype>({input_size});\n",
    "auto r = matx::make_tensor<dtype>({input_size});\n",
    "auto T = matx::make_tensor<dtype>({input_size});\n",
    "auto output = matx::make_tensor<dtype>({input_size});  \n",
    "\n",
    "// ---- populate the data ---- //\n",
    "(K = matx::random<float>(K.Shape(), matx::NORMAL)).run();\n",
    "(S = matx::random<float>(S.Shape(), matx::NORMAL)).run();\n",
    "(V = matx::random<float>(V.Shape(), matx::NORMAL)).run();\n",
    "(r = matx::random<float>(r.Shape(), matx::NORMAL)).run();\n",
    "(T = matx::random<float>(T.Shape(), matx::NORMAL)).run();\n",
    "(output = matx::zeros({input_size})).run(exec);\n",
    "exec.sync();\n",
    "\n",
    "auto VsqrtT = V * sqrt(T);\n",
    "auto d1     = (log(S / K) + (r + 0.5 * V * V) * T) / VsqrtT ;\n",
    "auto d2     = d1 - VsqrtT;\n",
    "auto cdf_d1 = matx::normcdf(d1);\n",
    "auto cdf_d2 = matx::normcdf(d2);\n",
    "auto expRT  = exp(-1 * r * T); \n",
    "exec.start_timer();\n",
    "(output = S * cdf_d1 - K * expRT * cdf_d2).run(exec);\n",
    "exec.stop_timer();\n",
    "std::cout <<\"Fused Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations & Intermediates\n",
    "some limitions exist that prevents the fusion of all operations. Like all CUDA programs, there is an upper ceiling to the complexity of how much compute is optimal for a given kernel, as the kernel's complexity drives resource utilization (such as registers and shared memory), that my ultimately harm performance.\n",
    "\n",
    "Similarly some lower-level APIs utilized by MatX may not support iterators / pre / post operations, and cannot be fused. \n",
    "\n",
    "To resolve this MatX uses Asnychronous memory when required to create intermediate outputs to store information between non-fusable operations. This does not require any action on the user to enable, however it may result in sub-optimal performance if asnchronous pools are not managed appropriately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  matx::index_t size_x = 12;\n",
    "  matx::index_t size_y = 12;\n",
    "  \n",
    "  // matx::index_t size_x = 128;\n",
    "  // matx::index_t size_y = 256;\n",
    "\n",
    "  auto A      = matx::make_tensor<cuda::std::complex<float>>({size_x, size_y});\n",
    "  auto B      = matx::make_tensor<cuda::std::complex<float>>({size_x, size_y});\n",
    "  auto result = matx::make_tensor<cuda::std::complex<float>>({size_x, size_y});\n",
    "\n",
    "  for (int i = 0; i < 10; i++) \n",
    "  {  \n",
    "    exec.start_timer();\n",
    "    (A = fft(A)).run(exec);\n",
    "    (A = A * B).run(exec);\n",
    "    (A = ifft(A)).run(exec);\n",
    "    exec.stop_timer();\n",
    "  }\n",
    "  std::cout <<\"NonFused Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;\n",
    "\n",
    "\n",
    "  for (int i = 0; i < 10; i++) \n",
    "  {  \n",
    "    exec.start_timer();\n",
    "    (A = ifft(fft(A)*B)).run(exec);\n",
    "    exec.stop_timer();\n",
    "  }\n",
    "  std::cout <<\"Fused Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;\n",
    "\n",
    "  for (int i = 0; i < 10; i++) \n",
    "  {  \n",
    "    exec.start_timer();\n",
    "    (A = fft(A*B)).run(exec);\n",
    "    (A = ifft(A)).run(exec);\n",
    "    exec.stop_timer();\n",
    "  }\n",
    "  std::cout <<\"Partial Fused Runtime: \" << exec.get_time_ms() << \" ms\" << std::endl;\n",
    "\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
