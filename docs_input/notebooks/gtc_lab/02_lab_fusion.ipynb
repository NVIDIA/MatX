{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation Fusion\n",
    "\n",
    "Using the typical order of operations rules, we evaluate the expression in parentheses first `(cos(C) / D)`, followed by the multiply, then the assignment. Written using standard C++ operator overloading, we would have a cosine, division, multiplication, and assignment overload. Each operator performs their respective task, then returns the value computed. That returned value is stored somewhere (either out to memory or possible in a register), then the next operator uses that output as input into its own computation. Finally, the assignment writes the value, usually out to memory.\n",
    "\n",
    "To avoid this overhead, MatX uses a technique called lazy evaluation to reduce the total number of loads and stores. It does this by overloading each operator so that **instead of performing the operation, such as multiplication, instead it returns an object that represents multiplication when itâ€™s needed.** The entire expression is generates a single type in C++ representing the equation above, and when we ask for element (0,0) of A above, the value is computed on-the-fly without storing any values. This also implies that you can store an entire expression into a variable and nothing will be exectuted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "//todo this should be moved to a hidden init block that runs automatically when the notebook starts\n",
    "#pragma cling add_library_path(\"/usr/local/cuda/lib64\")\n",
    "#pragma cling add_library_path(\"/opt/xeus/cling/lib\")\n",
    "//#pragma cling add_library_path(\"/usr/Lib/gcc/x86_64-Linux-gnu/11/\")\n",
    "#pragma cling add_library_path(\"/usr/lib/x86_64-linux-gnu/openblas64-openmp/\")\n",
    "#pragma cling add_include_path(\"/usr/local/cuda/include\")\n",
    "#pragma cling add_include_path(\"/usr/include/x86_64-linux-gnu/openblas64-openmp\")\n",
    "#pragma cling add_include_path(\"/opt/xeus/cling/tools/Jupyter/kernel/MatX/include\")\n",
    "#pragma cling add_include_path(\"/opt/xeus/cling/tools/Jupyter/kernel/MatX/build/_deps/cccl-src/libcudacxx/include\")\n",
    "//#pragma cling load(\"libgomp\")\n",
    "#pragma cling load(\"libopenblas64\")\n",
    "#pragma cling load(\"libcuda\")\n",
    "#pragma cling load(\"libcudart\")\n",
    "#pragma cling load(\"libcurand\")\n",
    "#pragma cling load(\"libcublas\")\n",
    "#pragma cling load(\"libcublasLt\")\n",
    "\n",
    "#include <cuda/std/__algorithm/max.h>\n",
    "#include <cuda/std/__algorithm/min.h>\n",
    "\n",
    "#define MATX_EN_OPENBLAS\n",
    "#define MATX_EN_OPENBLAS_LAPACK\n",
    "#define MATX_OPENBLAS_64BITINT\n",
    "\n",
    "#include \"matx.h\"\n",
    "\n",
    "exec = matx::SingleThreadedHostExecutor;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "auto op = (B * (cos(C) / D));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above op can be further combined with other expressions, which can increase code readability without loss of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Add Timeline showing set of ops and when they actually execute on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "size_t size_x = 128;\n",
    "size_t size_y = 256;\n",
    "\n",
    "auto A      = matx::make_tensor({size_x, size_y});\n",
    "auto B      = matx::make_tensor({size_x, size_y});\n",
    "auto C      = matx::make_tensor({size_x, size_y});\n",
    "auto D      = matx::make_tensor({size_x, size_y});\n",
    "auto result = matx::make_tensor({size_x, size_y});\n",
    "\n",
    "// first C++ code\n",
    "(result = A + B).run(exec);       // read A, read B,      write result\n",
    "(result = result + C).run(exec);  // read C, read result, write result\n",
    "(result += D).run(exec);          // read D, read result, write result\n",
    "// total memory accesses: read: 6 * size_x * size_y, write: 3 * size_x * size_y\n",
    "\n",
    "(result = A + B + C + D).run(exec);\n",
    "// total memory accesses: read: 4 * size_x * size_y, write: 1 * size_x * size_y\n",
    "// ~1/2 memory accesses compared to the previous code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Improvements\n",
    "The fused results in all of the performance benefits we described above:\n",
    "- a single kernel is submitted to the GPU to complete all operations\n",
    "- memory is only read from global once and written to global once\n",
    "\n",
    "# TODO show Exectutor timing of  how the fusion improved performance\n",
    "\n",
    "\n",
    "this results in X less memory traffic and Y faster kernel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "source": [
    "## Fusion with Operators\n",
    "\n",
    "Fusion is intuitive when all operands can be combined into a single statement, however fusion can also provide significant benefit for readability and reuse in implementations where very complex terms are defined, or even reused in the same solution. \n",
    "\n",
    "due to the lazy evaluation of operators, operator terms can be defined to clearly construct the specfic math for each term, and then these operators can be combined later to create the complete final expression for execution.\n",
    "\n",
    "Below we show a more complex operation comprised of both unary operators and transforms, and how we can break down a very complex expression into simple terms that can be reused\n",
    "\n",
    "# TODO: this is currently nonsense. do we want to update this to be more sensical?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "// Implementation 1: All terms together\n",
    "(result = A * C  + conv1d(B) / D + ((D - C) / B) / A * C).run(exec); \n",
    "\n",
    "\n",
    "// Implemenatation 2: break each term into it's own operation for readability\n",
    "(result = A*C).run(exec);\n",
    "(result += conv1d(B) / D).run(exec);\n",
    "(result += (D - C) / B).run(exec);\n",
    "\n",
    "// Implementation 3: create operators for each term, and then combine in single execution\n",
    "auto term1 = A * C; \n",
    "auto term2 = conv1d(B) / D;\n",
    "auto term3 = (D - C) / B;\n",
    "auto term4 = term3 / term1;\n",
    "(result = term1 + term2 + term4).run(exec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Improvements\n",
    "\n",
    "again we see the performance improvments of fusing the independent terms together, however we now also have the programming and readability benefit of code with well-defined terms.\n",
    "\n",
    "# TODO: include exectuion performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations & Intermediates\n",
    "some limitions exist that prevents the fusion of all operations. Like all CUDA programs, there is an upper ceiling to the complexity of how much compute is optimal for a given kernel, as the kernel's complexity drives resource utilization (such as registers and shared memory), that my ultimately harm performance.\n",
    "\n",
    "Similarly some lower-level APIs utilized by MatX may not support iterators / pre / post operations, and cannot be fused. \n",
    "\n",
    "To resolve this MatX uses Asnychronous memory when required to create intermediate outputs to store information between non-fusable operations. This does not require any action on the user to enable, however it may result in sub-optimal performance if asnchronous pools are not managed appropriately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// dims don't work as is\n",
    "// outline when we need to write output for sake of API (FFT, not sure what else?)\n",
    "result = (ifft(matmul(B, fft(A))) + C ) * D; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Black Scholes Fusion\n",
    "\n",
    "The Black Scholes model provides a fantastic example of a real-world set of equations that greatly benefits from operator fusion. Black Scholes provides both a complex set of expressions that provide significant readability improvements if expressed as individual expressions, but also benefits from fusion of its separate operational parts. Below is a brief description of the Black Scholes models and its composite terms:\n",
    "\n",
    "\n",
    "$$\n",
    "C(S_0, K, T) = S_0 \\,\\Phi\\bigl(d_1\\bigr) \\;-\\; K \\, e^{-rT} \\,\\Phi\\bigl(d_2\\bigr),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "d_1 = \\frac{\\ln\\!\\bigl(\\tfrac{S_0}{K}\\bigr) + \\bigl(r + \\tfrac{\\sigma^2}{2}\\bigr)T}{\\sigma \\sqrt{T}},\n",
    "\\quad\n",
    "d_2 = d_1 - \\sigma \\sqrt{T}.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- \\( S_0 \\) is the current stock price\n",
    "- \\( K \\) is the strike price\n",
    "- \\( T \\) is the time to maturity (in years)\n",
    "- \\( r \\) is the risk-free interest rate (annualized)\n",
    "- \\( \\sigma \\) is the volatility of the underlying stock (annualized)\n",
    "- \\( \\Phi(\\cdot) \\) is the cumulative distribution function (CDF) of the standard normal distribution\n",
    "\n",
    "\n",
    "\n",
    "We can easily translate this by expressing each of the terms defined above as separate MatX operators, then fusing the execution of those operators in the final run command.\n",
    "\n",
    "Try breaking the equation below into the following operators:\n",
    "\n",
    "```\n",
    "VsqrtT  = V * sqrt(T);\n",
    "d1      = (log(S / K) + (r + 0.5 * V * V) * T) / VsqrtT ;\n",
    "d2      = d1 - VsqrtT;\n",
    "cdf_d1  = normcdf(d1);\n",
    "cdf_d2  = normcdf(d2);\n",
    "expRT   = exp(-1 * r * T); \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "using dtype = double;\n",
    "index_t input_size = 10;\n",
    "// index_t inputIsze  = 10000000; // increase size to measure performance\n",
    "\n",
    "//declare input data\n",
    "auto K = matx::make_tensor<dtype>({input_size});\n",
    "auto S = matx::make_tensor<dtype>({input_size});\n",
    "auto V = matx::make_tensor<dtype>({input_size});\n",
    "auto r = matx::make_tensor<dtype>({input_size});\n",
    "auto T = matx::make_tensor<dtype>({input_size});\n",
    "auto output = matx::make_tensor<dtype>({input_size});  \n",
    "auto referenceOutput = matx::make_tensor<dtype>({input_size});  \n",
    "\n",
    "// Individually Evaluated Reference\n",
    "(referenceOutput = S * matx::normcdf((matx::log(S / K) + (r + 0.5 * V * V) * T) / V * matx::sqrt(T)) - K * matx::exp(-1 * r * T) * matx::normcdf((matx::log(S / K) + (r + 0.5 * V * V) * T) / V * sqrt(T) - V * sqrt(T);)).run(exec);\n",
    "print(referenceOutput);\n",
    "\n",
    "// well organized version\n",
    "// auto VsqrtT = V * sqrt(T);\n",
    "// auto d1     = (log(S / K) + (r + 0.5 * V * V) * T) / VsqrtT ;\n",
    "// auto d2     = d1 - VsqrtT;\n",
    "// auto cdf_d1 = normcdf(d1);\n",
    "// auto cdf_d2 = normcdf(d2);\n",
    "// auto expRT  = exp(-1 * r * T); \n",
    "\n",
    "// (output = S * cdf_d1 - K * expRT * cdf_d2).run(exec);\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
