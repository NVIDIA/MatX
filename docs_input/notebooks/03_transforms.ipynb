{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Numerical Computing with MatX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial List\n",
    "1. [Introduction](01_introduction.ipynb)\n",
    "2. [Operators](02_operators.ipynb)\n",
    "3. Executors (this tutorial)\n",
    "4. [Radar Pipeline Example](04_radar_pipeline.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executors\n",
    "MatX executors are a generic name given to functions that execute work on the device. Operators and generators were introduced in the last tutorial as a way to generate a CUDA kernel from an expression, but they do not execute any work on the device. The `run` took an operator as input and executed it on the device. Many other types of executors exist in MatX where more complex functions can be executed alongside operators. Some executors are wrappers around existing CUDA libraries, while others are custom executors inside of MatX. This distinction is hidden from developers so that the implementation of an executor can change over time without modifying the client code. Some executors can take an operator as input, while others can only take tensors as input. These restrictions are noted in the MatX documentation, and may be relaxed or removed in future versions.\n",
    "\n",
    "Besides `run`, other executors typically allow non-element-wise kernels to execute using highly-optimized library backends. Some examples of this would be a matrix multiply (GEMM), reduction, FFT, sorting, and linear solvers. Besides the type of inputs allowed, executors may also have restrictions on the rank and/or size of a tensor. For example, performing a GEMM requires that the tensors are at least rank 2 (i.e. be a matrix), and the last dimension of the first tensor must match the second-to-last dimension of the second tensor (`MxK * KxN`). Most executors support batching, and anything above the nominal rank will result in batching dimensions. In a 1D FFT this would mean that any dimension above 1 is treated as another 1D batched FFT, and a 2D FFT would batch any dimensions above 2. \n",
    "\n",
    "Some executors use CUDA libraries to implement their functionality, and those libraries require either a handle or a plan to operated. MatX hides this complexity by creating and caching the plan on the first call, and using the same plan on future calls where possible. More advanced users may use the handle interface directly to avoid the caching. Only the caching interface will be covered in this tutorial since it's the recommended approach, but the non-cached version can be found in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//todo this should be moved to a hidden init block that runs automatically when the notebook starts\n",
    "#pragma cling add_library_path(\"/usr/local/cuda/lib64\")\n",
    "#pragma cling add_library_path(\"/opt/xeus/cling/lib\")\n",
    "//#pragma cling add_library_path(\"/usr/Lib/gcc/x86_64-Linux-gnu/11/\")\n",
    "#pragma cling add_library_path(\"/usr/lib/x86_64-linux-gnu/openblas64-openmp/\")\n",
    "#pragma cling add_include_path(\"/usr/local/cuda/include\")\n",
    "#pragma cling add_include_path(\"/usr/include/x86_64-linux-gnu/openblas64-openmp\")\n",
    "#pragma cling add_include_path(\"/opt/xeus/cling/tools/Jupyter/kernel/MatX/include\")\n",
    "#pragma cling add_include_path(\"/opt/xeus/cling/tools/Jupyter/kernel/MatX/build/_deps/cccl-src/libcudacxx/include\")\n",
    "//#pragma cling load(\"libgomp\")\n",
    "#pragma cling load(\"libopenblas64\")\n",
    "#pragma cling load(\"libcuda\")\n",
    "#pragma cling load(\"libcudart\")\n",
    "#pragma cling load(\"libcurand\")\n",
    "#pragma cling load(\"libcublas\")\n",
    "#pragma cling load(\"libcublasLt\")\n",
    "#pragma cling load(\"libcufft\")\n",
    "\n",
    "#include <cuda/std/__algorithm/max.h>\n",
    "#include <cuda/std/__algorithm/min.h>\n",
    "\n",
    "#define MATX_EN_OPENBLAS\n",
    "#define MATX_EN_OPENBLAS_LAPACK\n",
    "#define MATX_OPENBLAS_64BITINT\n",
    "\n",
    "#include \"matx.h\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiply\n",
    "The `matmul` executor performs the matrix-matrix multiply of $$C = {\\alpha}A * B + {\\beta}C$$ where `A` is of dimensions `MxK`, `B` is `KxN`, and `C` is `MxN`. We first populate the `A` and `B` matrices with random values before the multiply as we did in the example above, then the GEMM is performed. Since the random number generator allocates memory sufficient to randomize the entire tensor, we create a random number generator large enough to generate values for both A or B. This allows us to create a single random number generator, but pull different random values for A and B by simply calling `run` twice. As mentioned above, any rank above 2 is consiered a batching dimension.\n",
    "\n",
    "We use rectangular matrices for `A` and `B`, while `C` will be a square matrix due to the outer dimensions of `A` and `B` matching. \n",
    "\n",
    "\n",
    "Open the file [exercises/example3_gemm.cu](exercises/example3_gemm.cu) and edit the contents where you see TODO markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor_2_f32: Tensor{float} Rank: 2, Sizes:[8, 4], Strides:[4,1]\n",
      "000000:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000001:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000002:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000003:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000004:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000005:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000006:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000007:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "B:\n",
      "tensor_2_f32: Tensor{float} Rank: 2, Sizes:[4, 8], Strides:[8,1]\n",
      "000000:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000001:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000002:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000003:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "C:\n",
      "tensor_2_f32: Tensor{float} Rank: 2, Sizes:[8, 8], Strides:[8,1]\n",
      "000000:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000001:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000002:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000003:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000004:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000005:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000006:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000007:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x78e507dfec30\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  auto A = matx::make_tensor<float>({8, 4});\n",
    "  auto B = matx::make_tensor<float>({4, 8});\n",
    "  auto C = matx::make_tensor<float>({8, 8});\n",
    "\n",
    "  (A = matx::random<float>({8, 4}, matx::NORMAL)).run();  \n",
    "  (B = matx::random<float>({4, 8}, matx::NORMAL)).run();  \n",
    "\n",
    "  // TODO: Perform a GEMM of C = A*B\n",
    "  (C = matx::matmul(A, B)).run();\n",
    "  \n",
    "  printf(\"A:\\n\");\n",
    "  matx::print(A);\n",
    "  printf(\"B:\\n\");\n",
    "  matx::print(B);  \n",
    "  printf(\"C:\\n\");\n",
    "  matx::print(C);    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT\n",
    "MatX provides an interface to do both 1D Fast Fourier Transforms (FFTs) and 2D FFTs. Any tensor above rank 1 will be batched in a 1D FFT, and any tensor above rank 2 will be batched in a 2D FFT. FFTs may either be done in-place or out-of-place by using the same or different variables for the output and inputs. Since the tensors are strongly-typed, the type of FFT (C2C, R2C, etc) is inferred by the tensor type at compile time. Similarly, the input and output size of the executor is deduced by the type of transform, and the input/output tensors must match those sizes. There's one exception to this rule, and it's when the input FFT is to be zero-padded at the end. In this case, the input tensor can be shorter than the output tensor, and the input will be zero-padded to the length of the output tensor. This is a common tactic used in signal and image processing for both speed and FFT resolution.\n",
    "\n",
    "In this example, we execute a 1D batched FFT on a 2D tensor populated with random complex floating point data. Since the FFT executor is performed in-place, the input and output types of the tensors are the same, and the type of the FFT is inferred as a complex-to-complex (`C2C`). The FFT length is specified by the inner dimension of the tensor, or 4 in this example, and the outer dimension is the number of batches, or 2. After the FFT completes, we perform on IFFT on the same tensor using the `ifft` interface. Ignoring floating point inaccuracies, the result of `ifft(fft(A))` should be the same as `A`, and this is shown by printing the tensors at each step. To perform a batched FFT on columns instead of rows, the tensor can be transposed by calling the `Permute` function used in the first tutorial. When the library detects a permuted tensor is being used, it can use technique to speed the FFT up over the naive method of converting the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto D = matx::make_tensor<float>({2, 4});\n",
    "\n",
    "// (D = matx::random<float>(D.Shape(), matx::NORMAL)).run();\n",
    "// matx::print(D);\n",
    "\n",
    "// (D = fft(D)).run();\n",
    "// matx::print(D);\n",
    "\n",
    "// (D = matx::ifft(D)).run();  \n",
    "// matx::print(D);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take the same 2D tensor and perform a 2D FFT on it. Since the rank is 2, it will not be batched as in the previous example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(void) @0x78e507dfec30\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(D = matx::random<float>(D.Shape(), matx::NORMAL)).run();\n",
    "// matx::print(D);\n",
    "\n",
    "// (D = fft2(D)).run();\n",
    "// matx::print(D);\n",
    "\n",
    "// (D = matx::ifft2(D)).run();  \n",
    "// matx::print(D);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the results after the IFFT closely match the original `C` tensor, but with floating point error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions\n",
    "A reduction operation takes multiple values and aggregates those into a smaller number of values. Most reductions take a large number of values and reduces them to a single value. Reductions are one of the most common operations perfomed on the GPU, which means they've been heavily researched and optimized for highly-parallel processors. Modern NVIDIA GPUs have special instructions for performing reductions to give even larger speedups over naive implementations. All of these details are hidden from the user and MatX automatically chooses the optimized path based on the hardware capabilities. \n",
    "\n",
    "MatX provides a set of optimized primitives to perform reductions on tensors for many common types. Reductions are supported across individual dimensions or on entire tensors, depending on the size of the output tensor. Currently supported reduction functions are `sum`, `min`, `max`,` mean`, `any`, and `all`.\n",
    "\n",
    "#### Full Reduction\n",
    "In this example we reduce an entire tensor to a single value by applying the reduction across all dimensions of the tensor. We apply the same random initialization from previous examples on a 2D tensor `A`. Note that the output tensor must be zeroed for a `sum` reduction since that value is continually added to during the reduction. Not initializing the output tensor will give undefined results since the variables are used as accumulators throughout the reduction. With the tensor initialized, we perform both a `max` and `sum` reduction across all dimensions of the tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor_2_f32: Tensor{float} Rank: 2, Sizes:[8, 4], Strides:[4,1]\n",
      "000000:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000001:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000002:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000003:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000004:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000005:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000006:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000007:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "Max: 0.000000\n",
      "Sum: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(int) 14\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  auto MD0 = matx::make_tensor<float>({});\n",
    "  auto AD0 = matx::make_tensor<float>({});\n",
    "\n",
    "  (A = matx::random<float>(A.Shape(), matx::NORMAL)).run();    \n",
    "  \n",
    "  // Initialize max and average to 0\n",
    "  (MD0 = 0).run();\n",
    "  (AD0 = 0).run();\n",
    "\n",
    "  (MD0 = max(A)).run();\n",
    "  (AD0 = sum(A)).run();\n",
    "\n",
    "  printf(\"A:\\n\");\n",
    "  matx::print(A);\n",
    "  printf(\"Max: %f\\n\", MD0());\n",
    "  printf(\"Sum: %f\\n\", AD0()); "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensional Reductions\n",
    "Reductions can also be performed across certain dimensions instead of the whole tensor. Dimensional reductions are useful in situations where each row contains data for a different user, for example, and we wish to sum up each user's data. By setting the output tensor view to a 1D tensor, independent reductions can be performed across the input tensor where each output element corresponds to a single row reduction from the input. Using the same tensor `A` from the previous example, we only change the output tensor type to be a 1D tensor instead of a scalar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(void) @0x78e507dfec30\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  auto MD1 = matx::make_tensor<float>({A.Size(0)});\n",
    "  auto AD1 = matx::make_tensor<float>({A.Size(0)});\n",
    "\n",
    "  (A = matx::random<float>(A.Shape(), matx::NORMAL)).run();    \n",
    "  \n",
    "  // Initialize max and average to 0\n",
    "  (MD1 = 0).run();\n",
    "  (AD1 = 0).run();\n",
    "\n",
    "  (MD1 = max(A)).run();\n",
    "  (AD1 = sum(A)).run();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the new reduction tensors shows the reduced values across each row of the input tensor `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor_2_f32: Tensor{float} Rank: 2, Sizes:[8, 4], Strides:[4,1]\n",
      "000000:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000001:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000002:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000003:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000004:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000005:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000006:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000007:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "Max:\n",
      "tensor_1_f32: Tensor{float} Rank: 1, Sizes:[8], Strides:[1]\n",
      "000000:  0.0000e+00 \n",
      "000001:  0.0000e+00 \n",
      "000002:  0.0000e+00 \n",
      "000003:  0.0000e+00 \n",
      "000004:  0.0000e+00 \n",
      "000005:  0.0000e+00 \n",
      "000006:  0.0000e+00 \n",
      "000007:  0.0000e+00 \n",
      "Sum:\n",
      "tensor_1_f32: Tensor{float} Rank: 1, Sizes:[8], Strides:[1]\n",
      "000000:  0.0000e+00 \n",
      "000001:  0.0000e+00 \n",
      "000002:  0.0000e+00 \n",
      "000003:  0.0000e+00 \n",
      "000004:  0.0000e+00 \n",
      "000005:  0.0000e+00 \n",
      "000006:  0.0000e+00 \n",
      "000007:  0.0000e+00 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x78e507dfec30\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  printf(\"A:\\n\");\n",
    "  matx::print(A);\n",
    "  printf(\"Max:\\n\");\n",
    "  matx::print(MD1);\n",
    "  printf(\"Sum:\\n\");\n",
    "  matx::print(AD1);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "MatX supports both 1D and 2D direct convolution using the `conv1d` and `conv2d` functions. FFT-based convolution can also be performed as a combination of existing primitives as a potentially faster alternative to direct convolution for large tensors. Both forms of direct convolution take in an extra mode which specifies how much of the output is saved, where `MATX_C_MODE_FULL` saves the entire filter ramp-up and down, `MATX_C_MODE_SAME` makes the input and output tensors the same size, and `MATX_C_MODE_VALID` only keeps valid samples (when the entire filter was part of the convolution). Convolution can be used to perform a rolling average of an input by making all filter values 1/N, where N is the length of the filter. In this example, we use a filter of length 3 to create a running average of the last 3 elements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CIn tensor:\n",
      "tensor_1_f32: Tensor{float} Rank: 1, Sizes:[16], Strides:[1]\n",
      "000000:  0.0000e+00 \n",
      "000001:  0.0000e+00 \n",
      "000002:  0.0000e+00 \n",
      "000003:  0.0000e+00 \n",
      "000004:  0.0000e+00 \n",
      "000005:  0.0000e+00 \n",
      "000006:  0.0000e+00 \n",
      "000007:  0.0000e+00 \n",
      "000008:  0.0000e+00 \n",
      "000009:  0.0000e+00 \n",
      "000010:  0.0000e+00 \n",
      "000011:  0.0000e+00 \n",
      "000012:  0.0000e+00 \n",
      "000013:  0.0000e+00 \n",
      "000014:  0.0000e+00 \n",
      "000015:  0.0000e+00 \n",
      "tensor_1_f32: Tensor{float} Rank: 1, Sizes:[18], Strides:[1]\n",
      "000000:  0.0000e+00 \n",
      "000001:  0.0000e+00 \n",
      "000002:  0.0000e+00 \n",
      "000003:  0.0000e+00 \n",
      "000004:  0.0000e+00 \n",
      "000005:  0.0000e+00 \n",
      "000006:  0.0000e+00 \n",
      "000007:  0.0000e+00 \n",
      "000008:  0.0000e+00 \n",
      "000009:  0.0000e+00 \n",
      "000010:  0.0000e+00 \n",
      "000011:  0.0000e+00 \n",
      "000012:  0.0000e+00 \n",
      "000013:  0.0000e+00 \n",
      "000014:  0.0000e+00 \n",
      "000015:  0.0000e+00 \n",
      "000016:  0.0000e+00 \n",
      "000017:  0.0000e+00 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x78e507dfec30\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto CIn  = matx::make_tensor<float>({16});\n",
    "auto filt = matx::make_tensor<float>({3});\n",
    "auto Co   = matx::make_tensor<float>({16 + filt.Lsize() - 1});\n",
    "\n",
    "filt.SetVals({1.0/3, 1.0/3, 1.0/3});\n",
    "\n",
    "(CIn = matx::random<float>({16}, matx::NORMAL)).run();  \n",
    "\n",
    "printf(\"Initial CIn tensor:\\n\");\n",
    "matx::print(CIn);\n",
    "(Co = matx::conv1d(CIn, filt, matx::MATX_C_MODE_FULL)).run();\n",
    "\n",
    "matx::print(Co);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to a 1D convolution, a 2D convolution does the same computation over two dimensions. A tensor of at least rank 2 is needed for a 2D convolution. Below we use a filter of all ones using the `ones` operator to demonstrate the filter can also be an operator and not an existing tensor view. The result is the sum of the four values around each cell on the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial C tensor:\n",
      "tensor_2_f32: Tensor{float} Rank: 2, Sizes:[8, 8], Strides:[8,1]\n",
      "000000:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000001:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000002:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000003:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000004:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000005:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000006:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000007:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "After conv2d:\n",
      "tensor_2_f32: Tensor{float} Rank: 2, Sizes:[8, 8], Strides:[8,1]\n",
      "000000:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000001:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000002:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000003:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000004:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000005:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000006:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n",
      "000007:  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(void) @0x78e507dfec30\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  auto CIn2  = matx::make_tensor<float>({8,8});\n",
    "  auto filt2 = matx::ones<float>({2, 2});\n",
    "  auto Co2   = matx::make_tensor<float>({8, 8});\n",
    "\n",
    "  (CIn2 = matx::random<float>({8, 8}, matx::NORMAL)).run();  \n",
    "\n",
    "  printf(\"Initial C tensor:\\n\");\n",
    "  matx::print(C);\n",
    "\n",
    "  (Co2 = matx::conv2d(CIn2, filt, matx::MATX_C_MODE_SAME)).run();\n",
    "  \n",
    "  printf(\"After conv2d:\\n\");\n",
    "  matx::print(Co2);\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we mentioned above that convolution can also be done in the frequency domain using FFTs. This is the preferred method for larger tensors since FFTs are much faster than direct convolutions in large sizes, and because FFT libraries are highly-optimized. FFT convolution uses more memory than direct if the inputs are not to be destroyed since it requires running an FFT on both the input signal and filter before filtering. If not done in-place, this typically requires `2N + L - 1` new elements in memory, where N is the signal length and L is the filter length. A full FFT convolution example can be found in `fft_conv.cu` in the MatX examples, but the main convolution code is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "///\\todo complete this tutorial. This one is pretty complex, do we want to keep the validation code here?\n",
    "\n",
    "// using complex = cuda::std::complex<float>;\n",
    "// cudaExecutor exec{};\n",
    "\n",
    "// index_t signal_size = 16;\n",
    "// index_t filter_size = 3;\n",
    "// index_t filtered_size = signal_size + filter_size - 1;\n",
    "\n",
    "// // Create time domain buffers\n",
    "// auto sig_time  = make_tensor<complex>({signal_size});\n",
    "// auto filt_time = make_tensor<complex>({filter_size});\n",
    "// auto time_out  = make_tensor<complex>({filtered_size});\n",
    "\n",
    "// // Frequency domain buffers\n",
    "// auto sig_freq  = make_tensor<complex>({filtered_size});\n",
    "// auto filt_freq = make_tensor<complex>({filtered_size});\n",
    "\n",
    "// // Fill the time domain signals with data\n",
    "// for (index_t i = 0; i < signal_size; i++) {\n",
    "//   sig_time(i) = {-1.0f * (2.0f * static_cast<float>(i % 2) + 1.0f) *\n",
    "//                         (static_cast<float>(i % 10) / 10.0f) +\n",
    "//                     0.1f,\n",
    "//                 -1.0f * (static_cast<float>(i % 2) == 0.0f) *\n",
    "//                         (static_cast<float>(i % 10) / 5.0f) -\n",
    "//                     0.1f};\n",
    "// }\n",
    "// for (index_t i = 0; i < filter_size; i++) {\n",
    "//   filt_time(i) = {static_cast<float>(i) / static_cast<float>(filter_size),\n",
    "//                   static_cast<float>(-i) / static_cast<float>(filter_size) +\n",
    "//                       0.5f};\n",
    "// }\n",
    "\n",
    "// TODO: Perform FFT convolution\n",
    "// Perform the FFT in-place on both signal and filter\n",
    "// (sig_freq = fft(sig_time)).run();\n",
    "// (filt_freq = fft(filt_time)).run();\n",
    "\n",
    "// (sig_freq = sig_freq * filt_freq).run();\n",
    "\n",
    "// // IFFT in-place\n",
    "// (sig_freq = ifft(sig_freq)).run();  \n",
    "\n",
    "\n",
    "// Perform the FFT in-place on both signal and filter, do an element-wise multiply of the two, then IFFT that output\n",
    "// (sig_freq = ifft(fft(sig_time, filtered_size) * fft(filt_time, filtered_size))).run(stream);\n",
    "\n",
    "// TODO: Perform a time-domain convolution\n",
    "// conv1d(time_out, sig_time, filt_time, matxConvCorrMode_t::MATX_C_MODE_FULL, 0);\n",
    "\n",
    "// exec.sync();\n",
    "\n",
    "// // Compare signals\n",
    "// for (index_t i = 0; i < filtered_size; i++) {\n",
    "//     if (  fabs(time_out(i).real() - sig_freq(i).real()) > 0.001 || \n",
    "//           fabs(time_out(i).imag() - sig_freq(i).imag()) > 0.001) {\n",
    "//         printf(\"Verification failed at item %lld. Direct=%f%+.2fj, FFT=%f%+.2fj\\n\", i,\n",
    "//           time_out(i).real(), time_out(i).imag(), sig_freq(i).real(), sig_freq(i).imag());\n",
    "//         return -1;\n",
    "//     }\n",
    "// }\n",
    "\n",
    "std::cout << \"Verification successful\" << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the expected output size of the full filtering operation is signal_len + filter_len - 1, both the filter and signal time domain inputs are shorter than the output. This would normally require a separate stage of allocating buffers of the appropriate size, zeroing them out, copying the time domain data to the buffers, and performing the  FFT. However, MatX has an API to do all of this automatically in the library using asynchronous allocations. This makes the call have a noticeable performance hit on the first call, but subsequent calls will be close to the time without allocation. To recognize that automatic padding is wanted, MatX uses the output tensor size compared to the input tensor size to determine whether to pad the input with zeros. In this case the output signal (sig_time and filt_time) are shorter than the output tensors (sig_freq and filt_freq), so it will automatically zero-pad the input.\n",
    "\n",
    "The above expression can also be combined into a single line:\n",
    "\n",
    "```c++\n",
    "(sig_freq = ifft(fft(sig_time, filtered_size) * fft(filt_time, filtered_size))).run(stream);\n",
    "```\n",
    "\n",
    "Using the convolution property $ h*x \\leftrightarrow H \\cdot X$ we simply multiply the signals element-wise after the FFT, then do an IFFT to go back to the time domain.\n",
    "\n",
    "Next, we do the same operation in the time domain using the `conv1d` function:\n",
    "\n",
    "```c++\n",
    "conv1d(time_out, sig_time, filt_time, matxConvCorrMode_t::MATX_C_MODE_FULL, 0);\n",
    "```\n",
    "\n",
    "To match the FFT results we do a full convolution to get all the samples from the filter ramp up and ramp down. However, if we wanted either valid or same mode we could slice the FFT convolution output at the appropriate places to give the same answer. Edit the file [exercises/example3_fft_conv.cu](exercises/example3_fft_conv.cu) and add the missing code where you see TODOs. After running the verification code at the bottom will check for accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the third tutorial on MatX. In this tutorial you learned what executors are, and how they can be applied on tensor views. In the next example you will walk through an entire radar signal processing pipeline using all the primites learned up to this point. \n",
    "\n",
    "[Start Next Tutorial](04_radar_pipeline.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++",
   "name": "cling-cpp17"
  },
  "language_info": {
   "codemirror_mode": "c++",
   "file_extension": ".c++",
   "mimetype": "text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
